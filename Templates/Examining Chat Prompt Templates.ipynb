{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatPromptTemplate\n",
    "ChatPromptTemplate is a powerful tool in LangChain that allows you to define structured prompts for conversational AI models. It enables you to create prompts with multiple message types and provides flexibility for complex conversational scenarios.\n",
    "\n",
    "Key characteristics of ChatPromptTemplate:\n",
    "\n",
    "- Supports multiple message types (system, human, AI)\n",
    "- Can include context and role-specific instructions\n",
    "- More flexible for complex conversational scenarios\n",
    "- Outputs a list of messages\n",
    "- Better suited for chat models that expect a sequence of messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Aspect               | PromptTemplate                          | ChatPromptTemplate                          |\n",
    "|-----------------------|-----------------------------------------|---------------------------------------------|\n",
    "| **Message Structure** | Single string                          | Multiple message types (system, human, AI) |\n",
    "| **Use Cases**         | Simple, single-turn interactions       | Multi-turn conversations, context-rich interactions |\n",
    "| **Model Compatibility** | Works with traditional language models | Designed for chat-based models             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### When to Use Each\n",
    "\n",
    "**Use `PromptTemplate` for:**\n",
    "- Simple, single-turn queries\n",
    "- Traditional language models\n",
    "- Straightforward text generation\n",
    "\n",
    "**Use `ChatPromptTemplate` for:**\n",
    "- Conversational AI\n",
    "- Multi-turn interactions\n",
    "- Scenarios requiring context or role-based instructions\n",
    "- Chat models like GPT-3.5, GPT-4\n",
    "\n",
    "The choice depends on your specific use case and the type of language model you're using.\n",
    "\n",
    "For more details, please visit [Prompt Templates Documentation](https://python.langchain.com/docs/concepts/prompt_templates/) and [Chat Models Documentation](https://python.langchain.com/docs/concepts/chat_models/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "base_url = \"http://localhost:11434\"\n",
    "model = 'llama3.2:1b'\n",
    "llm = ChatOllama(\n",
    "    base_url=base_url,\n",
    "    model=model,\n",
    "    temperature=0.5,\n",
    "    num_predict=512\n",
    "    #timeout=None,\n",
    "    # max_retries=2,\n",
    "    # api_key=\"...\",  # if you prefer to pass api key in directly instaed of using env vars\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Messages in Chat Models\n",
    "\n",
    "Messages are the fundamental units of communication in chat models, representing inputs, outputs, and any associated context or metadata. Each message includes:\n",
    "\n",
    "- **Role**: Defines the message's origin (e.g., \"user\", \"assistant\").\n",
    "- **Content**: Contains the message data (e.g., text, multimodal data).\n",
    "- **Metadata**: Additional information that varies by chat model provider.\n",
    "\n",
    "LangChain offers a unified message format, enabling seamless interaction with various chat models without concern for provider-specific message formats. \n",
    "| **Role**          | **Description**                                                                                                                                   |\n",
    "|--------------------|---------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **system**         | Used to tell the chat model how to behave and provide additional context. Not supported by all chat model providers.                              |\n",
    "| **user**           | Represents input from a user interacting with the model, usually in the form of text or other interactive input.                                 |\n",
    "| **assistant**      | Represents a response from the model, which can include text or a request to invoke tools.                                                       |\n",
    "| **tool**           | A message used to pass the results of a tool invocation back to the model after external data or processing has been retrieved.                  |\n",
    "| **function (legacy)** | This is a legacy role, corresponding to OpenAI's legacy function-calling API. `tool` role should be used instead.                              |\n",
    "\n",
    "For more details, please visit [LangChain Messages Documentation](https://python.langchain.com/docs/concepts/messages).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create a ChatPromptTemplate with multiple message types\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant specialized in explaining technical concepts.\"),\n",
    "    (\"human\", \"Please explain {topic} using exactly {number} detailed and distinct use cases.\")\n",
    "])\n",
    "\n",
    "# Invoke the chat prompt with specific values\n",
    "formatted_chat_prompt = chat_prompt.invoke({\n",
    "    \"topic\": \"langchain\", \n",
    "    \"number\": 2\n",
    "})\n",
    "chat_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the model with the formatted prompt\n",
    "response = llm.invoke(formatted_chat_prompt)\n",
    "\n",
    "# Print the response content\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same example as above but with conversational context\n",
    "\n",
    "# Create a ChatPromptTemplate that simulates a more interactive conversation\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    # Role , Message\n",
    "    # System message sets the overall context and behavior\n",
    "    # SystemMessage(content=\"You are a helpful AI assistant who explains technical concepts in a friendly, approachable manner.\"),\n",
    "    (\"system\", \"You are a helpful AI assistant who explains technical concepts in a friendly, approachable manner.\"),\n",
    "    \n",
    "    # First human message sets up the initial context\n",
    "    # HumanMessage(content=\"Can you help me understand what {topic} is?\"),\n",
    "    (\"human\", \"Can you help me understand what {topic} is?\"),\n",
    "    \n",
    "    # An example AI response (this helps provide context for the model)\n",
    "    # AiMessage(content=\"{topic} is a powerful framework for developing applications powered by large language models. It provides tools to help chain together different components of AI applications.\"),\n",
    "    (\"ai\", \"{topic} is a powerful framework for developing applications powered by large language models. It provides tools to help chain together different components of AI applications.\"),\n",
    "    \n",
    "    # Another human follow-up message\n",
    "    # HumanMessage(content=\"Please explain {topic} using exactly {number} detailed and distinct use cases.\")\n",
    "    (\"human\", \"Please explain {topic} using exactly {number} detailed and distinct use cases.\")\n",
    "])\n",
    "\n",
    "# Invoke the prompt with specific values\n",
    "formatted_chat_prompt = chat_prompt.invoke({\n",
    "    \"number\": 5,\n",
    "    \"topic\": \"langchain\"\n",
    "})\n",
    "\n",
    "formatted_chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(formatted_chat_prompt).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, you can use the `invoke` method directly on the ChatPromptTemplate using LCEL\n",
    "chain = chat_prompt | llm\n",
    "chain.invoke({ \"number\": 2,\n",
    "    \"topic\": \"langchain\"}).content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
