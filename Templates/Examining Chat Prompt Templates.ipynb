{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatPromptTemplate\n",
    "ChatPromptTemplate is a powerful tool in LangChain that allows you to define structured prompts for conversational AI models. It enables you to create prompts with multiple message types and provides flexibility for complex conversational scenarios.\n",
    "\n",
    "Key characteristics of ChatPromptTemplate:\n",
    "\n",
    "- Supports multiple message types (system, human, AI)\n",
    "- Can include context and role-specific instructions\n",
    "- More flexible for complex conversational scenarios\n",
    "- Outputs a list of messages\n",
    "- Better suited for chat models that expect a sequence of messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Aspect               | PromptTemplate                          | ChatPromptTemplate                          |\n",
    "|-----------------------|-----------------------------------------|---------------------------------------------|\n",
    "| **Message Structure** | Single string                          | Multiple message types (system, human, AI) |\n",
    "| **Use Cases**         | Simple, single-turn interactions       | Multi-turn conversations, context-rich interactions |\n",
    "| **Model Compatibility** | Works with traditional language models | Designed for chat-based models             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### When to Use Each\n",
    "\n",
    "**Use `PromptTemplate` for:**\n",
    "- Simple, single-turn queries\n",
    "- Traditional language models\n",
    "- Straightforward text generation\n",
    "\n",
    "**Use `ChatPromptTemplate` for:**\n",
    "- Conversational AI\n",
    "- Multi-turn interactions\n",
    "- Scenarios requiring context or role-based instructions\n",
    "- Chat models like GPT-3.5, GPT-4\n",
    "\n",
    "The choice depends on your specific use case and the type of language model you're using.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "base_url = \"http://localhost:11434\"\n",
    "model = 'llama3.2:1b'\n",
    "llm = ChatOllama(\n",
    "    base_url=base_url,\n",
    "    model=model,\n",
    "    temperature=0.5,\n",
    "    num_predict=512\n",
    "    #timeout=None,\n",
    "    # max_retries=2,\n",
    "    # api_key=\"...\",  # if you prefer to pass api key in directly instaed of using env vars\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create a ChatPromptTemplate with multiple message types\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant specialized in explaining technical concepts.\"),\n",
    "    (\"human\", \"Please explain {topic} using exactly {number} detailed and distinct use cases.\")\n",
    "])\n",
    "\n",
    "# Invoke the chat prompt with specific values\n",
    "formatted_chat_prompt = chat_prompt.invoke({\n",
    "    \"topic\": \"langchain\", \n",
    "    \"number\": 2\n",
    "})\n",
    "chat_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the model with the formatted prompt\n",
    "response = llm.invoke(formatted_chat_prompt)\n",
    "\n",
    "# Print the response content\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same example as above but with conversational context\n",
    "\n",
    "# Create a ChatPromptTemplate that simulates a more interactive conversation\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    # Role , Message\n",
    "    # System message sets the overall context and behavior\n",
    "    (\"system\", \"You are a helpful AI assistant who explains technical concepts in a friendly, approachable manner.\"),\n",
    "    \n",
    "    # First human message sets up the initial context\n",
    "    (\"human\", \"Can you help me understand what {topic} is?\"),\n",
    "    \n",
    "    # An example AI response (this helps provide context for the model)\n",
    "    (\"ai\", \"{topic} is a powerful framework for developing applications powered by large language models. It provides tools to help chain together different components of AI applications.\"),\n",
    "    \n",
    "    # Another human follow-up message\n",
    "    (\"human\", \"Please explain {topic} using exactly {number} detailed and distinct use cases.\")\n",
    "])\n",
    "\n",
    "# Invoke the prompt with specific values\n",
    "formatted_chat_prompt = chat_prompt.invoke({\n",
    "    \"number\": 5,\n",
    "    \"topic\": \"langchain\"\n",
    "})\n",
    "\n",
    "formatted_chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(formatted_chat_prompt).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, you can use the `invoke` method directly on the ChatPromptTemplate using LCEL\n",
    "chain = chat_prompt | llm\n",
    "chain.invoke({ \"number\": 2,\n",
    "    \"topic\": \"langchain\"}).content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
