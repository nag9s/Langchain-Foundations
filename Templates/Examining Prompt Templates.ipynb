{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Templates\n",
    "Prompt Templates are a powerful way to define reusable and parameterized prompts for language models. They allow you to create dynamic prompts by using placeholders that can be replaced with specific values at runtime. This approach simplifies the process of generating customized prompts for various use cases.\n",
    "\n",
    "In contrast, Langchain Chat Prompt Templates are specifically designed for multi-turn conversations. They allow you to define structured prompts that include both system and user messages, making them ideal for chat-based interactions. While Prompt Templates focus on single-turn prompts with placeholders, Chat Prompt Templates provide additional flexibility for managing conversational context.\n",
    "\n",
    "Below, we discuss two methods to work with Prompt Templates:\n",
    "1. Using the `from_template()` method for quick and convenient creation of templates.\n",
    "2. Creating a `PromptTemplate` object explicitly with input variables for more customization.\n",
    "\n",
    "Key Characteristics of PromptTemplate\n",
    "- Works with a single string template  \n",
    "- Uses simple `{variable}` placeholders  \n",
    "- Best for straightforward, single-message prompts  \n",
    "- Outputs a single formatted string  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "base_url = \"http://localhost:11434\"\n",
    "model = 'llama3.2:1b'\n",
    "llm = ChatOllama(\n",
    "    base_url=base_url,\n",
    "    model=model,\n",
    "    temperature=0.5,\n",
    "    num_predict=512\n",
    "    #timeout=None,\n",
    "    # max_retries=2,\n",
    "    # api_key=\"...\",  # if you prefer to pass api key in directly instaed of using env vars\n",
    "   \n",
    ")\n",
    "\n",
    "# 'Please explain {topic} using exactly {number} detailed and distinct use cases.'\n",
    "# response = llm.invoke('Please explain langchain using exactly 2 detailed and distinct use cases.')\n",
    "\n",
    "#print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1 : Using the from_template() method\n",
    "The `from_template()` method is used to create a `PromptTemplate` object by providing a template string with placeholders. In this case, the placeholders `{topic}` and `{number}` are defined in the template string. These placeholders can later be replaced with actual values using the `invoke()` method.\n",
    "\n",
    "The created `PromptTemplate` object allows for dynamic formatting of prompts by substituting the placeholders with specific values. This is particularly useful for generating customized prompts for language models.\n",
    "\n",
    "For example:\n",
    "- The template string `\"Please explain {topic} using exactly {number} detailed and distinct use cases.\"` is passed to `from_template()`.\n",
    "- The resulting `PromptTemplate` object can then be used to generate a formatted prompt by providing values for `topic` and `number`.\n",
    "\n",
    "This approach simplifies the process of creating reusable and parameterized prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Import the PromptTemplate class from langchain_core.prompts\n",
    "\n",
    "# Create a prompt template with placeholders for topic and number\n",
    "prompt_template = PromptTemplate.from_template(\"Please explain {topic} using exactly {number} detailed and distinct use cases.\")\n",
    "\n",
    "# Format the prompt by providing values for the placeholders\n",
    "formatted_prompt = prompt_template.invoke({\"topic\": \"langchain\", \"number\": 2})\n",
    "\n",
    "# Print the formatted prompt\n",
    "print(formatted_prompt)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Invoke the model with the formatted prompt\n",
    "response = llm.invoke(formatted_prompt)\n",
    "\n",
    "# Print the response content\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, you can use the prompt_template as a chain using LCEL\n",
    "\n",
    "# Create chain\n",
    "chain = (\n",
    "     prompt_template\n",
    "    | llm\n",
    ")\n",
    "# Invoke the chain\n",
    "response = chain.invoke({\"topic\": \"langchain\", \"number\": 2})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2. Creating a PromptTemplate Object with Explicit Input Variables\n",
    "The code creates a `PromptTemplate` object and a prompt simultaneously by directly defining the template string and input variables in the constructor of the `PromptTemplate` class. Unlike the `from_template()` method, which is a convenience method for creating a `PromptTemplate` object from a single template string, this approach provides more flexibility by allowing you to explicitly specify the `input_variables` and other parameters during initialization.\n",
    "\n",
    "This method is particularly useful when you want to define additional attributes or customize the behavior of the `PromptTemplate` object beyond what the `from_template()` method offers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define template\n",
    "template = \"Please explain {topic} using exactly {number} detailed and distinct use cases.\"\n",
    "\n",
    "# Create a prompt template with `PromptTemplate` object\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"topic\", \"number\"],\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompt\n",
    "# Format the prompt by providing values for the placeholders\n",
    "formatted_prompt = prompt.invoke({\"topic\": \"langchain\", \"number\": 2})\n",
    "formatted_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the model with the formatted prompt\n",
    "response = llm.invoke(formatted_prompt)\n",
    "\n",
    "# Print the response content\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Alternatively, you can use the prompt_template as a chain using LCEL\n",
    "chain = (\n",
    "    prompt\n",
    "    | llm\n",
    ")\n",
    "# Invoke the chain\n",
    "response = chain.invoke({\"topic\": \"langchain\", \"number\": 2})\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
