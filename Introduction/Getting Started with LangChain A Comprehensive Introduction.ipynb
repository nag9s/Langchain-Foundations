{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with LangChain A Comprehensive Introduction\n",
    "\n",
    "## Overview\n",
    "This notebook provides a comprehensive introduction to LangChain, showcasing a powerful framework for building intelligent, modular applications powered by large language models (LLMs). It demonstrates core capabilities for creating dynamic, context-aware AI workflows.\n",
    "\n",
    "## Key Features:\n",
    "- Flexible LLM interaction using ChatOllama\n",
    "- Dynamic prompt engineering\n",
    "- Streaming response generation\n",
    "- Declarative chain composition with LCEL\n",
    "- Modular AI workflow design\n",
    "\n",
    "## Technologies Used:\n",
    "- LangChain\n",
    "- Ollama LLM\n",
    "- LangChain Expression Language (LCEL)\n",
    "- Prompt Templates\n",
    "- Streaming AI Responses\n",
    "\n",
    "## Use Cases:\n",
    "- Intelligent content generation\n",
    "- Dynamic prompt-based AI interactions\n",
    "- Modular AI workflow development\n",
    "- Real-time AI response handling\n",
    "- Flexible language model integration\n",
    "\n",
    "\n",
    "## Activities Covered in This Notebook\n",
    "\n",
    "1. **Setting Up LangChain**  \n",
    "    - Importing necessary modules and initializing the environment for LangChain.\n",
    "\n",
    "2. **Using the `ChatOllama` Class**  \n",
    "    - Demonstrating how to interact with the Ollama LLM using the `ChatOllama` class.\n",
    "\n",
    "3. **Prompt Engineering**  \n",
    "    - Creating dynamic prompts using the `PromptTemplate` class.\n",
    "    - Formatting prompts with placeholders for runtime values.\n",
    "\n",
    "4. **Streaming Responses**  \n",
    "    - Exploring how to stream responses from the LLM for real-time interaction.\n",
    "\n",
    "5. **Using LCEL (LangChain Expression Language)**  \n",
    "    - Understanding the declarative approach to composing chains in LangChain.\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "This notebook provides a foundational understanding of LangChain and its core functionalities. In upcoming notebooks, we will explore advanced topics, including:\n",
    "\n",
    "- **Runnable Interfaces**: Understanding how to create reusable and modular components for LangChain workflows.\n",
    "- **Parallel Execution**: Running multiple chains or tasks concurrently for efficiency.\n",
    "- **Sequential Execution of Chains**: Structuring chains to execute in a defined sequence.\n",
    "- **Output Parsers**: Parsing and structuring the output from LLMs for specific use cases.\n",
    "- **LCEL (LangChain Expression Language)**: Diving deeper into the declarative approach for composing chains.\n",
    "- **Various Templates**: Exploring templates like Chat Templates and Prompt Chat Templates for dynamic interactions.\n",
    "\n",
    "Stay tuned for more detailed discussions and hands-on examples!\n",
    "\n",
    "\n",
    "> **Sidenote:** Ensure that you have selected the kernel as the conda environment named `langchain`, as instructed in Lab Guide 1. This is crucial for running the code in this notebook without any issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv('../env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "# os.environ['LANGSMITH_ENDPOINT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Importing the ChatOllama Class**\n",
    "   - The `ChatOllama` class is used to interact with the Ollama LLM (Large Language Model) API.\n",
    "\n",
    "2. **Defining the Base URL and Model**\n",
    "   - **`base_url`**: Specifies the URL where the Ollama server is running or cloud llm api endpoint is running. In this case, it is hosted locally on port `11434`.\n",
    "   - **`model`**: Defines the specific LLM model to use. Here, the model is `llama3.2:1b`.\n",
    "\n",
    "3. **Creating an Instance of ChatOllama**\n",
    "   - An instance of the `ChatOllama` class is created with the following parameters:\n",
    "     - **`base_url`**: The URL of the Ollama server.\n",
    "     - **`model`**: The LLM model to use.\n",
    "     - **`temperature`**: Controls the randomness of the model's output. A value of `0.8` allows for some creativity while maintaining coherence.\n",
    "     - **`num_predict`**: Specifies the maximum number of tokens to predict in the response.\n",
    "\n",
    "4. **Invoking the Model with a Prompt**\n",
    "   - The `invoke` method sends a prompt to the LLM and retrieves the response.\n",
    "   - The prompt in this case is: *\"Please explain langchain using 5 simple usecases.\"*\n",
    "\n",
    "5. **Printing the Response**\n",
    "   - The `print` function is used to display the content of the response received from the LLM.\n",
    "   - The `response.content` contains the text generated by the model based on the provided prompt.\n",
    "\n",
    "- This code demonstrates how to use the `ChatOllama` class to interact with an Ollama LLM.\n",
    "- It initializes the model with specific parameters, sends a prompt, and prints the generated response.\n",
    "- This is useful for generating natural language explanations or other text-based outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "base_url = \"http://localhost:11434\"\n",
    "model = 'llama3.2:1b'\n",
    "llm = ChatOllama(\n",
    "    base_url=base_url,\n",
    "    model=model,\n",
    "    temperature=0.5,\n",
    "    num_predict=512\n",
    "    #timeout=None,\n",
    "    # max_retries=2,\n",
    "    # api_key=\"...\",  # if you prefer to pass api key in directly instaed of using env vars\n",
    "   \n",
    ")\n",
    "\n",
    "# 'Please explain {topic} using exactly {number} detailed and distinct use cases.'\n",
    "response = llm.invoke('Please explain langchain using exactly 2 detailed and distinct use cases.')\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Responses in LangChain\n",
    "Streaming refers to the process of receiving data in small, incremental chunks rather than waiting for the entire response to be available. This is particularly useful when interacting with large language models (LLMs) that generate lengthy outputs, as it allows you to process and display the response in real-time.\n",
    "\n",
    "In the context of LangChain and the `ChatOllama` class, streaming enables you to retrieve and handle the model's output as it is being generated. This can improve user experience by providing immediate feedback and reducing perceived latency.\n",
    "\n",
    "The following code demonstrates how streaming works:\n",
    "\n",
    "```python\n",
    "response = \"\"\n",
    "for chunk in llm.stream('Please explain langchain using exactly 2 detailed and distinct use cases.'):\n",
    "    response = response + \" \" + chunk.content\n",
    "    print(response)\n",
    "```\n",
    "1. **`llm.stream()`**: This method sends a prompt to the LLM and returns a generator that yields chunks of the response as they are generated.\n",
    "\n",
    "2. **Iterating Over Chunks**: The `for` loop iterates over each chunk of the response. Each chunk contains a portion of the generated text.\n",
    "\n",
    "3. **Building the Response**: The `response` variable accumulates the content of all chunks to form the complete response.\n",
    "\n",
    "4. **Real-Time Output**: The `print(response)` statement displays the response incrementally, providing real-time feedback as the model generates the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = \"\"\n",
    "for chunk in llm.stream('Please explain langchain using exactly 2 detailed and distinct use cases.'):\n",
    "    response = response + \" \" + chunk.content\n",
    "    print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and Formatting Dynamic Prompts\n",
    "\n",
    "## What is a PromptTemplate?\n",
    "The PromptTemplate is a utility class used to create dynamic and reusable prompts for language models. It allows you to define a template with placeholders and then populate those placeholders with specific values at runtime.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define the base URL and model\n",
    "base_url = \"http://localhost:11434\"\n",
    "model = 'llama3.2:1b'\n",
    "\n",
    "# Create an instance of ChatOllama\n",
    "llm = ChatOllama(\n",
    "    base_url=base_url,\n",
    "    model=model,\n",
    "    temperature=0.5,\n",
    "    num_predict=512\n",
    ")\n",
    "\n",
    "# Create a PromptTemplate\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"number\"],\n",
    "    template=\"Please explain {topic} using exactly {number} detailed and distinct use cases.\"\n",
    ")\n",
    "\n",
    "# Format the prompt with dynamic values\n",
    "formatted_prompt = prompt_template.format(topic=\"langchain\", number=2)\n",
    "\n",
    "# Invoke the model with the formatted prompt\n",
    "response = llm.invoke(formatted_prompt)\n",
    "\n",
    "# Print the response content\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composing Chains with LCEL\n",
    "\n",
    "This activity demonstrates how to use LCEL to compose and execute chains in LangChain.\n",
    "LCEL (LangChain Expression Language) is a declarative way to compose chains in LangChain that allows you to describe what should happen, rather than how it should happen.\n",
    "\n",
    "``` python\n",
    "# Old approach (imperative)\n",
    "formatted_prompt = prompt_template.format(topic=\"langchain\", number=2)\n",
    "response = llm.invoke(formatted_prompt)\n",
    "\n",
    "# LCEL approach (declarative)\n",
    "chain = prompt | llm\n",
    "response = chain.invoke({\"topic\": \"langchain\", \"number\": 2})\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "# Define the base URL and model\n",
    "base_url = \"http://localhost:11434\"\n",
    "model = 'llama3.2:1b'\n",
    "\n",
    "# Create an instance of ChatOllama\n",
    "llm = ChatOllama(\n",
    "    base_url=base_url,\n",
    "    model=model,\n",
    "    temperature=0.5,\n",
    "    num_predict=512\n",
    ")\n",
    "\n",
    "# Create a PromptTemplate using LCEL\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Please explain {topic} using exactly {number} detailed and distinct use cases.\"\n",
    ")\n",
    "\n",
    "# Compose the chain using LCEL pipe operator\n",
    "chain = (\n",
    "    prompt \n",
    "    | llm\n",
    ")\n",
    "\n",
    "# Invoke the chain with input values\n",
    "response = chain.invoke({\n",
    "    \"topic\": \"langchain\", \n",
    "    \"number\": 2\n",
    "})\n",
    "\n",
    "# Print the response content\n",
    "print(response.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
