{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv('../env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['LANGSMITH_ENDPOINT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of the Code\n",
    "\n",
    "1. **Importing the ChatOllama Class**\n",
    "   - The `ChatOllama` class is used to interact with the Ollama LLM (Large Language Model) API.\n",
    "\n",
    "2. **Defining the Base URL and Model**\n",
    "   - **`base_url`**: Specifies the URL where the Ollama server is running. In this case, it is hosted locally on port `11434`.\n",
    "   - **`model`**: Defines the specific LLM model to use. Here, the model is `llama3.2:1b`.\n",
    "\n",
    "3. **Creating an Instance of ChatOllama**\n",
    "   - An instance of the `ChatOllama` class is created with the following parameters:\n",
    "     - **`base_url`**: The URL of the Ollama server.\n",
    "     - **`model`**: The LLM model to use.\n",
    "     - **`temperature`**: Controls the randomness of the model's output. A value of `0.8` allows for some creativity while maintaining coherence.\n",
    "     - **`num_predict`**: Specifies the maximum number of tokens to predict in the response.\n",
    "\n",
    "4. **Invoking the Model with a Prompt**\n",
    "   - The `invoke` method sends a prompt to the LLM and retrieves the response.\n",
    "   - The prompt in this case is: *\"Please explain langchain using 5 simple usecases.\"*\n",
    "\n",
    "5. **Printing the Response**\n",
    "   - The `print` function is used to display the content of the response received from the LLM.\n",
    "   - The `response.content` contains the text generated by the model based on the provided prompt.\n",
    "\n",
    "### Summary\n",
    "- This code demonstrates how to use the `ChatOllama` class to interact with an Ollama LLM.\n",
    "- It initializes the model with specific parameters, sends a prompt, and prints the generated response.\n",
    "- This is useful for generating natural language explanations or other text-based outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "base_url = \"http://localhost:11434\"\n",
    "model = 'llama3.2:1b'\n",
    "llm = ChatOllama(\n",
    "    base_url=base_url,\n",
    "    model=model,\n",
    "    temperature=0.5,\n",
    "    num_predict=512\n",
    "    #timeout=None,\n",
    "    # max_retries=2,\n",
    "    # api_key=\"...\",  # if you prefer to pass api key in directly instaed of using env vars\n",
    "   \n",
    ")\n",
    "\n",
    "# 'Please explain {topic} using exactly {number} detailed and distinct use cases.'\n",
    "response = llm.invoke('Please explain langchain using exactly 2 detailed and distinct use cases.')\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = \"\"\n",
    "# for chunk in llm.stream('Please explain langchain using 5 usecases'):\n",
    "#     response = response + \" \" + chunk.content\n",
    "#     print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.response_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Prompt Template\n",
    "## What is a PromptTemplate?\n",
    "The PromptTemplate is a utility class used to create dynamic and reusable prompts for language models. It allows you to define a template with placeholders and then populate those placeholders with specific values at runtime.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define the base URL and model\n",
    "base_url = \"http://localhost:11434\"\n",
    "model = 'llama3.2:1b'\n",
    "\n",
    "# Create an instance of ChatOllama\n",
    "llm = ChatOllama(\n",
    "    base_url=base_url,\n",
    "    model=model,\n",
    "    temperature=0.5,\n",
    "    num_predict=512\n",
    ")\n",
    "\n",
    "# Create a PromptTemplate\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"number\"],\n",
    "    template=\"Please explain {topic} using exactly {number} detailed and distinct use cases.\"\n",
    ")\n",
    "\n",
    "# Format the prompt with dynamic values\n",
    "formatted_prompt = prompt_template.format(topic=\"langchain\", number=2)\n",
    "\n",
    "# Invoke the model with the formatted prompt\n",
    "response = llm.invoke(formatted_prompt)\n",
    "\n",
    "# Print the response content\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LCEL\n",
    "LCEL (LangChain Expression Language) is a declarative way to compose chains in LangChain that allows you to describe what should happen, rather than how it should happen.\n",
    "\n",
    "``` python\n",
    "# Old approach (imperative)\n",
    "formatted_prompt = prompt_template.format(topic=\"langchain\", number=5)\n",
    "response = llm.invoke(formatted_prompt)\n",
    "\n",
    "# LCEL approach (declarative)\n",
    "chain = prompt | llm\n",
    "response = chain.invoke({\"topic\": \"langchain\", \"number\": 5})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "# Define the base URL and model\n",
    "base_url = \"http://localhost:11434\"\n",
    "model = 'llama3.2:1b'\n",
    "\n",
    "# Create an instance of ChatOllama\n",
    "llm = ChatOllama(\n",
    "    base_url=base_url,\n",
    "    model=model,\n",
    "    temperature=0.5,\n",
    "    num_predict=512\n",
    ")\n",
    "\n",
    "# Create a PromptTemplate using LCEL\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Please explain {topic} using exactly {number} detailed and distinct use cases.\"\n",
    ")\n",
    "\n",
    "# Compose the chain using LCEL pipe operator\n",
    "chain = (\n",
    "    prompt \n",
    "    | llm\n",
    ")\n",
    "\n",
    "# Invoke the chain with input values\n",
    "response = chain.invoke({\n",
    "    \"topic\": \"langchain\", \n",
    "    \"number\": 2\n",
    "})\n",
    "\n",
    "# Print the response content\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
