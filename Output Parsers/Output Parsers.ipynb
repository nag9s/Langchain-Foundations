{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "709ca7be",
   "metadata": {},
   "source": [
    "# Output Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6caf2db-0362-41d4-8e63-871d9112e6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "base_url = \"http://localhost:11434\"\n",
    "model = 'llama3.2:1b'\n",
    "# Initialize the Llama model \n",
    "llm = ChatOllama(\n",
    "    base_url=base_url,\n",
    "    model = model,\n",
    "    temperature = 0.8,\n",
    "    num_predict = 256\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9093729e",
   "metadata": {},
   "source": [
    "# Chain Processing without / with the output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27680941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Without Parser ---\n",
      "Type: <class 'langchain_core.messages.ai.AIMessage'>\n",
      "Content: I can help you with that. However, I need to know a bit more information from you. Could you please specify what time of year or date you're interested in finding out about the weather in San Francisco?\n",
      "\n",
      "Is it a sunny day, an overcast day, a rainy day, or a chilly evening? Are you looking for the average high and low temperatures, precipitation, or something else specific?\n",
      "\n",
      "--- With StrOutputParser ---\n",
      "Type: <class 'str'>\n",
      "Content: I can help you with that. However, I'm a large language model, I don't have real-time access to current weather conditions. But I can provide you with general information about the typical weather patterns in San Francisco.\n",
      "\n",
      "San Francisco is located near the Pacific Ocean and is known for its mild climate. The city experiences a Mediterranean climate, characterized by mild, wet winters and cool, dry summers.\n",
      "\n",
      "Here's what you can expect the weather to be like during different times of the year:\n",
      "\n",
      "* Winter (December to February): Cool and wet, with average high temperatures around 58°F (14°C) and average low temperatures around 45°F (7°C).\n",
      "* Spring (March to May): Mild and pleasant, with average high temperatures ranging from 62°F (17°C) to 67°F (19°C) and average low temperatures ranging from 52°F (11°C) to 55°F (13°C).\n",
      "* Summer (June to August): Cool and dry, with average high temperatures around 65°F (18°C) and average low temperatures around 50°F (10°C).\n",
      "* Autumn (September to November): Mild and pleasant, with average high temperatures ranging from 62°F (17°C) to 67°F (19°C) and average low\n"
     ]
    }
   ],
   "source": [
    "# Without parser\n",
    "print(\"--- Without Parser ---\")\n",
    "response = llm.invoke(\"What's the weather in San Francisco?\")\n",
    "print(\"Type:\", type(response))\n",
    "print(\"Content:\", response.content)\n",
    "\n",
    "\n",
    "# With StrOutputParser\n",
    "print(\"\\n--- With StrOutputParser ---\")\n",
    "chain = llm | StrOutputParser()\n",
    "response = chain.invoke(\"What's the weather in San Francisco?\")\n",
    "print(\"Type:\", type(response))\n",
    "print(\"Content:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6649b76-ce6c-4b44-8599-0887ed01921a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "\n",
    "\n",
    "# Create a prompt template for financial transaction analysis\n",
    "transaction_analysis_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Analyze the following payment transaction details:\n",
    "    - Transaction Amount: {amount}\n",
    "    - Merchant: {merchant}\n",
    "    - Transaction Type: {transaction_type}\n",
    "\n",
    "    Provide a detailed risk assessment and potential fraud indicators.\"\"\"\n",
    ")\n",
    "\n",
    "# Create a RunnableSequence to process the transaction\n",
    "transaction_analysis_chain = (\n",
    "    transaction_analysis_prompt  # First, format the prompt\n",
    "    | llm  # Then, pass to the Llama model for analysis\n",
    "    \n",
    ")\n",
    "\n",
    "# Example usage\n",
    "def analyze_transaction(amount, merchant, transaction_type):\n",
    "    result = transaction_analysis_chain.invoke({\n",
    "        \"amount\": amount,\n",
    "        \"merchant\": merchant,\n",
    "        \"transaction_type\": transaction_type\n",
    "    })\n",
    "    return result\n",
    "\n",
    "# Demonstrate the chain\n",
    "transaction_result = analyze_transaction(\n",
    "    amount=\"$1,250.75\", \n",
    "    merchant=\"TechCorp Online Store\", \n",
    "    transaction_type=\"E-commerce Purchase\"\n",
    ")\n",
    "\n",
    "print(\"Transaction Analysis:\")\n",
    "print(transaction_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61556929",
   "metadata": {},
   "source": [
    "# CommaSeparatedListOutputParser"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
